{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gender_classifier.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenduytung74913/bai-tap-ve-nha-deep-learning/blob/main/gender_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6FnixIvCdP5"
      },
      "source": [
        "try:\n",
        "  # Use the %tensorflow_version magic if in colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "  \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, BatchNormalization, Input, Activation, MaxPooling2D, Flatten\n",
        "import os\n",
        "import dlib\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import datetime\n",
        "import plotly.express as px\n",
        "from random import randint\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "%load_ext tensorboard\n",
        "\n",
        "project_path = \"drive/My Drive/Colab Notebooks/tutorial.Gender Classifier\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXdhJRPlCpN7"
      },
      "source": [
        "data = open(project_path+\"/dataset/wiki_data.pickle\", \"rb\")\n",
        "data = pickle.load(data)\n",
        "\n",
        "images = np.array(data['images'])\n",
        "gender = np.array(data['gender'])\n",
        "print('''The shape of the images array is : {}\\n\n",
        "The shape is an image is : {}\\n\n",
        "The shape of the gender array is : {}'''.format(images.shape, images[0].shape, gender.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeSzsW0aWXyc"
      },
      "source": [
        "plt.figure()\n",
        "i = 1\n",
        "\n",
        "while(i <= 4):\n",
        "    index = randint(0, len(images))\n",
        "    img = images[index]\n",
        "    \n",
        "    plt.subplot(1, 4, i)\n",
        "    plt.imshow(img)\n",
        "    plt.title(gender[index][0])\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BRxnYroWP3Z"
      },
      "source": [
        "gender_plotting = []\n",
        "\n",
        "for i in gender:\n",
        "    if i[0] == 1:\n",
        "        gender_plotting.append('Male')\n",
        "    else:\n",
        "        gender_plotting.append('Female')\n",
        "\n",
        "dataframe = pd.DataFrame({'gender' : gender_plotting})\n",
        "fig = px.histogram(dataframe, x=\"gender\")\n",
        "fig.show()\n",
        "del dataframe\n",
        "del gender_plotting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lB2ojL7XHU9"
      },
      "source": [
        "genderCategorical = []\n",
        "\n",
        "for i in gender:\n",
        "    if i[0] == 1:\n",
        "        genderCategorical.append([1.0, 0.0])\n",
        "    else:\n",
        "        genderCategorical.append([0.0, 1.0])\n",
        "genderCategorical = np.array(genderCategorical)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25pCNB4pfKgV"
      },
      "source": [
        "%tensorboard --logdir \"drive/My Drive/Colab Notebooks/Tutorial/Gender Classifier/logs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUqAhgFHR-dg"
      },
      "source": [
        "layers = [Input(shape=(32,32,3))]\n",
        "no_of_conv_layers = (16,32, 64,128)\n",
        "\n",
        "for i in no_of_conv_layers:\n",
        "    layers += [\n",
        "            Conv2D(i, padding='same', kernel_size=(2,2)),\n",
        "               Activation('relu'),\n",
        "               BatchNormalization(),\n",
        "               MaxPooling2D(pool_size=(2,2), strides=2)\n",
        "    ]\n",
        "\n",
        "layers += [\n",
        "           Flatten(),\n",
        "           Dense(512),\n",
        "           Activation('relu'),\n",
        "           BatchNormalization(),\n",
        "           Dropout(0.25),\n",
        "           Dense(128),\n",
        "           Activation('relu'),\n",
        "           BatchNormalization(),\n",
        "           Dropout(0.25),\n",
        "           Dense(64),\n",
        "           Activation('relu'),\n",
        "           BatchNormalization(),\n",
        "           Dense(16),\n",
        "           Activation('relu'),\n",
        "           Dense(2),\n",
        "           Activation('softmax')\n",
        "]\n",
        "\n",
        "model = tf.keras.Sequential(layers)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "log_dir = \"drive/My Drive/Colab Notebooks/Tutorial/Gender Classifier/logs/\" + datetime.datetime.now().strftime(\"%Y%m%D-%H%M%S\")\n",
        "# log_dir = \"logs/\"+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "callbacks = [TensorBoard(log_dir=log_dir)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3Squ7A9JdI1"
      },
      "source": [
        "train_images, test_images, train_gender, test_gender = train_test_split(images, genderCategorical, \n",
        "                                                    test_size = .2, shuffle = True, random_state = 10)\n",
        "\n",
        "num_train_examples = len(train_images) * 0.8\n",
        "BATCH_SIZE = 64\n",
        "model.fit(train_images, train_gender, epochs = 10, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),  \n",
        "          batch_size = BATCH_SIZE, shuffle=True, validation_split = 0.2,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "print(\"\\nEvaluating the Model\\n\")\n",
        "model.evaluate(test_images, test_gender, callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3WKIdll7ZcC"
      },
      "source": [
        "def rgba2rgb( rgba, background=(255,255,255) ):\n",
        "    row, col, ch = rgba.shape\n",
        "\n",
        "    if ch == 3:\n",
        "        return rgba\n",
        "\n",
        "    assert ch == 4, 'RGBA image has 4 channels.'\n",
        "\n",
        "    rgb = np.zeros( (row, col, 3), dtype='float32' )\n",
        "    r, g, b, a = rgba[:,:,0], rgba[:,:,1], rgba[:,:,2], rgba[:,:,3]\n",
        "\n",
        "    a = np.asarray( a, dtype='float32' ) / 255.0\n",
        "\n",
        "    R, G, B = background\n",
        "\n",
        "    rgb[:,:,0] = r * a + (1.0 - a) * R\n",
        "    rgb[:,:,1] = g * a + (1.0 - a) * G\n",
        "    rgb[:,:,2] = b * a + (1.0 - a) * B\n",
        "\n",
        "    return np.asarray( rgb, dtype='uint8' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLldg_bL4kLG"
      },
      "source": [
        "def take_photo( img_width = 48, img_height = 48, quality=0.8 ):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(img_width, img_height, quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await  navigator.mediaDevices.getUserMedia({video: {height:img_height, width:img_width}});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      ctx = canvas.getContext('2d')\n",
        "      ctx.drawImage(video, 0, 0);\n",
        "      imageData  = ctx.getImageData(1,1, Math.round(img_width), Math.round(img_height))\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return imageData.data\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({},{},{})'.format(img_width, img_height, quality))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHpqr8Sh5vcg"
      },
      "source": [
        "img_height = 256\n",
        "img_width = 256\n",
        "\n",
        "try:\n",
        "    data = take_photo(img_height, img_width, 0.3)\n",
        "    img = []\n",
        "    for i in sorted(data, key = lambda x : int(x)):\n",
        "        img.append(data[i])\n",
        "    img = np.reshape(img, (img_height, img_width,4))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))\n",
        "\n",
        "img = rgba2rgb(img)\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWgnz0aV7PN1"
      },
      "source": [
        "def extract_faces(img):\n",
        "    cnn_face_detector = dlib.cnn_face_detection_model_v1(project_path+\"/dlib_cnn_weights.dat\")\n",
        "    faces_cnn = cnn_face_detector(img, 1)\n",
        "    faces = []\n",
        "    if(len(faces_cnn) > 0):\n",
        "        for face in faces_cnn:\n",
        "            offset_x , offset_y  = max(face.rect.left(),0),max(face.rect.top(),0)\n",
        "            target_width, target_height = face.rect.right() - offset_x, face.rect.bottom() - offset_y\n",
        "\n",
        "            target_width = min(target_width, img.shape[1]-offset_x)\n",
        "            target_height = min(target_height, img.shape[0]-offset_y)\n",
        "            print(\"face\", offset_x,offset_y,target_width,target_height)\n",
        "\n",
        "            face_img = tf.image.crop_to_bounding_box(img, \n",
        "                                            offset_y, offset_x, \n",
        "                                            target_height,target_width)\n",
        "\n",
        "            face_img = tf.image.resize(face_img, (32, 32), method=tf.image.ResizeMethod.BICUBIC, antialias=True)\n",
        "            face_img = tf.dtypes.cast(face_img, tf.int32)\n",
        "            faces.append({'face' : face_img.numpy(), 'coordinates' : [offset_x, offset_y, target_width, target_height]})\n",
        "    return faces\n",
        "\n",
        "def show_output(img, faces, predictions):\n",
        "    for i, data in enumerate(faces):\n",
        "        coordinates = data['coordinates']\n",
        "        x1 = coordinates[0]\n",
        "        y1 = coordinates[1]\n",
        "        x2 = coordinates[2] + x1\n",
        "        y2 = coordinates[3] + y1\n",
        "        cv2.rectangle(img, (x1, y1), (x2, y2), (0,0,255), 1)\n",
        "        gender = 'Male' if np.argmax(predictions[i]) == 0 else 'Female'\n",
        "        cv2.putText(img, gender, (x1-3, y1), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,0,0), 1)\n",
        "    plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK4xtD6f-bDM"
      },
      "source": [
        "faces = extract_faces(img)\n",
        "predict_X = []\n",
        "for face in faces:\n",
        "    predict_X.append(face['face'])\n",
        "\n",
        "predict_X = np.array(predict_X)\n",
        "\n",
        "predictions = []\n",
        "if predict_X.shape[0] > 0 :\n",
        "    predictions = model.predict(predict_X)\n",
        "\n",
        "show_output(img, faces, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bài 2 tuần 4",
      "provenance": [],
      "authorship_tag": "ABX9TyPKkjYIlFV+yLPDoql5FIcl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenduytung74913/bai-tap-ve-nha-deep-learning/blob/main/b%C3%A0i_2_tu%E1%BA%A7n_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfymXl5ja3x2",
        "outputId": "05d8507d-1e5c-48dd-9214-626aa0e69ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "import tensorflow as tf\n",
        " \n",
        " # Input data \n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"./data/MNIST_data\", one_hot=True)\n",
        " \n",
        " # Define network hyperparameters\n",
        "learning_rate = 0.0001\n",
        "training_iters = 400000\n",
        "batch_size = 256\n",
        "display_step = 5\n",
        " \n",
        " # Define network parameters\n",
        "N_input = 784 # Enter the dimension (img shape: 28*28)\n",
        "N_classes = 10 #labeled dimensions (0-9 digits)\n",
        "Dropout = 0.5 # probability of Dropout, possibility of output\n",
        " \n",
        " # Enter placeholder\n",
        "x = tf.placeholder(tf.float32, [None, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)   # dropout (keep probability)\n",
        " \n",
        " \n",
        " # Define convolution operations\n",
        "def conv2d(name, x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x, name=name) # Use relu to activate the function\n",
        " \n",
        " \n",
        " # Define pooling layer operations\n",
        "def maxpool2d(name, x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME', name=name)\n",
        " \n",
        " \n",
        " # Normalized operation\n",
        "def norm(name, l_input, lsize=4):\n",
        "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0,\n",
        "                     beta=0.75, name=name)\n",
        " \n",
        " # Define all network parameters\n",
        "weights = {\n",
        "    'wc1': tf.Variable(tf.random_normal([11, 11, 1, 96])),\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 96, 256])),\n",
        "    'wc3': tf.Variable(tf.random_normal([3, 3, 256, 384])),\n",
        "    'wc4': tf.Variable(tf.random_normal([3, 3, 384, 384])),\n",
        "    'wc5': tf.Variable(tf.random_normal([3, 3, 384, 256])),\n",
        "    'wd1': tf.Variable(tf.random_normal([4*4*256, 4096])),\n",
        "    'wd2': tf.Variable(tf.random_normal([4096, 4096])),\n",
        "    'out': tf.Variable(tf.random_normal([4096, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([96])),\n",
        "    'bc2': tf.Variable(tf.random_normal([256])),\n",
        "    'bc3': tf.Variable(tf.random_normal([384])),\n",
        "    'bc4': tf.Variable(tf.random_normal([384])),\n",
        "    'bc5': tf.Variable(tf.random_normal([256])),\n",
        "    'bd1': tf.Variable(tf.random_normal([4096])),\n",
        "    'bd2': tf.Variable(tf.random_normal([4096])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        " \n",
        " \n",
        " # Define the entire network\n",
        "def alex_net(x, weights, biases, dropout):\n",
        "         # Vector to matrix Reshape input picture\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        " \n",
        "         # \n",
        "         # convolution\n",
        "    conv1 = conv2d('conv1', x, weights['wc1'], biases['bc1'])\n",
        "         # \n",
        "    pool1 = maxpool2d('pool1', conv1, k=2)\n",
        "         # \n",
        "    norm1 = norm('norm1', pool1, lsize=4)\n",
        " \n",
        "         # \n",
        "         # convolution\n",
        "    conv2 = conv2d('conv2', norm1, weights['wc2'], biases['bc2'])\n",
        "         #Maximum pooling (downsampling)\n",
        "    pool2 = maxpool2d('pool2', conv2, k=2)\n",
        "         # \n",
        "    norm2 = norm('norm2', pool2, lsize=4)\n",
        " \n",
        "         # third layer convolution\n",
        "         # convolution\n",
        "    conv3 = conv2d('conv3', norm2, weights['wc3'], biases['bc3'])\n",
        "         # \n",
        "    norm3 = norm('norm3', conv3, lsize=4)\n",
        " \n",
        "         # \n",
        "    conv4 = conv2d('conv4', norm3, weights['wc4'], biases['bc4'])\n",
        " \n",
        "         # fifth layer convolution\n",
        "    conv5 = conv2d('conv5', conv4, weights['wc5'], biases['bc5'])\n",
        "         #Maximum pooling (downsampling)\n",
        "    pool5 = maxpool2d('pool5', conv5, k=2)\n",
        "         # \n",
        "    norm5 = norm('norm5', pool5, lsize=4)\n",
        " \n",
        "         # 1\n",
        "    fc1 = tf.reshape(norm5, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 =tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # dropout\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        " \n",
        "         # 2\n",
        "    fc2 = tf.reshape(fc1, [-1, weights['wd2'].get_shape().as_list()[0]])\n",
        "    fc2 = tf.add(tf.matmul(fc2, weights['wd2']), biases['bd2'])\n",
        "    fc2 = tf.nn.relu(fc2)\n",
        "    # dropout\n",
        "    fc2=tf.nn.dropout(fc2, dropout)\n",
        " \n",
        "         #output layer\n",
        "    out = tf.add(tf.matmul(fc2, weights['out']), biases['out'])\n",
        "    return out\n",
        " \n",
        " #Build model\n",
        "pred = alex_net(x, weights, biases, keep_prob)\n",
        " \n",
        " # Define loss function and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        " \n",
        " # \n",
        "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        " \n",
        " # Initialize variables \n",
        "init = tf.global_variables_initializer()\n",
        " \n",
        "import os\n",
        "ckpt_dir = './tmp/ckpt_dir'\n",
        "if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        " # Define an extractor\n",
        "saver = tf.train.Saver()\n",
        " \n",
        " # Open a training\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    step = 1\n",
        "         #Start training until training_iters is reached, ie 200000\n",
        "    while step * batch_size < training_iters:\n",
        "                 # Get batch data\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
        "        if step % display_step == 0:\n",
        "                         # Calculate the loss value and accuracy, output\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
        "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
        "        step += 1\n",
        "    print(\"Optimization Finished!\")\n",
        "         # Calculate the accuracy of the test set\n",
        "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}))\n",
        "    saver.save(sess, './tmp/ckpt_dir/model.ckpt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e37b75aa6e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m  \u001b[0;31m# Input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/MNIST_data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples.tutorials'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}